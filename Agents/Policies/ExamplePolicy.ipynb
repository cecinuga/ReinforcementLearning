{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 17:44:12.167342: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-07 17:44:12.294755: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-07 17:44:12.616014: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-07 17:44:12.616084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-07 17:44:12.637700: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-07 17:44:12.692797: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-07 17:44:12.695372: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-07 17:44:15.511010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import network\n",
    "\n",
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import actor_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Policies\n",
    "The interface for Python policies is defined in policies/py_policy.PyPolicy. The main methods are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Random Python Policy\n",
    "\n",
    "A simple example of a PyPolicy is the RandomPyPolicy, which generates random actions for the discrete/continuous given action_spec. The input time_step is ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyStep(action=array([ 8, -3], dtype=int32), state=(), info=())\n",
      "PolicyStep(action=array([-6,  6], dtype=int32), state=(), info=())\n"
     ]
    }
   ],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "my_random_py_policy = random_py_policy.RandomPyPolicy(time_step_spec=None, action_spec=action_spec)\n",
    "time_step = None\n",
    "action_spec = my_random_py_policy.action(time_step)\n",
    "print(action_spec)\n",
    "action_spec = my_random_py_policy.action(time_step)\n",
    "print(action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Scripted Python Policy\n",
    "\n",
    "A scripted policy plays back a script of actions represented as a list of (num_repeats, action) tuples. Every time the action function is called, it returns the next action from the list until the specified number of repeats is done, and then moves on to the next action in the list. The reset method can be called to start executing from the beginning of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing scripted policy...\n",
      "PolicyStep(action=array([2, 5], dtype=int32), state=[0, 1], info=())\n",
      "PolicyStep(action=array([1, 2], dtype=int32), state=[2, 1], info=())\n",
      "PolicyStep(action=array([1, 2], dtype=int32), state=[2, 2], info=())\n",
      "\n",
      "Resetting my_scripted_py_policy...\n",
      "PolicyStep(action=array([2, 5], dtype=int32), state=[0, 1], info=())\n"
     ]
    }
   ],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "action_script = [\n",
    "    ( 1, np.array( [2, 5], dtype=np.int32 )),\n",
    "    ( 0, np.array( [0, 0], dtype=np.int32 )),\n",
    "    ( 2, np.array( [1, 2], dtype=np.int32 )),\n",
    "    ( 1, np.array( [2, 5], dtype=np.int32 )),\n",
    "]\n",
    "\n",
    "my_scripted_py_policy = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=None, action_spec=action_spec, action_script=action_script\n",
    ")\n",
    "\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "time_step = None\n",
    "\n",
    "print('Executing scripted policy...')\n",
    "action_step = my_scripted_py_policy.action(time_step, policy_state)\n",
    "print(action_step)\n",
    "action_step= my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "action_step = my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "\n",
    "print('\\nResetting my_scripted_py_policy...')\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "action_step = my_scripted_py_policy.action(time_step, policy_state)\n",
    "print(action_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Policies\n",
    "TensorFlow policies follow the same interface as Python policies. Let us look at a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Random TF Policy\n",
    "A RandomTFPolicy can be used to generate random actions according to a given discrete/continuous action_spec. The input time_step is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [-0.21865416  1.0914135 ]\n"
     ]
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec((2,), tf.float32, minimum=-1, maximum=3)\n",
    "input_tensor_spec = tensor_spec.TensorSpec((2,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "\n",
    "my_random_tf_policy = random_tf_policy.RandomTFPolicy(\n",
    "    action_spec=action_spec, time_step_spec=time_step_spec\n",
    ")\n",
    "observation = tf.ones(time_step_spec.observation.shape)\n",
    "time_step = ts.restart(observation)\n",
    "action_step = my_random_tf_policy.action(time_step)\n",
    "\n",
    "print(f'Action: {action_step.action}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Actor Policy\n",
    "An actor policy can be created using either a network that maps time_steps to actions or a network that maps time_steps to distributions over actions.\n",
    "\n",
    "### Using an action network: \n",
    "Let us define a network as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNet(network.Network):\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super(ActionNet, self).__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='ActionNet'\n",
    "        )\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._sub_layers = [\n",
    "            tf.keras.layers.Dense(\n",
    "                action_spec.shape.num_elements(), activation=tf.nn.tanh\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        def call(self, observations, step_type, network_state):\n",
    "            del step_type\n",
    "\n",
    "            output = tf.cast(observations, dtype=tf.float32)\n",
    "            for layer in self._sub_layers:\n",
    "                output = layer(output)\n",
    "\n",
    "            actions = tf.reshape(output, [-1] + self._output_tensor_spec.shape.as_list())          \n",
    "            return actions, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)\n  In call to configurable 'ActorPolicy' (<class 'tf_agents.policies.actor_policy.ActorPolicy'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m action_spec \u001b[38;5;241m=\u001b[39m tensor_spec\u001b[38;5;241m.\u001b[39mBoundedTensorSpec((\u001b[38;5;241m3\u001b[39m,), tf\u001b[38;5;241m.\u001b[39mfloat32, minimum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, maximum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m action_net \u001b[38;5;241m=\u001b[39m ActionNet(input_tensor_spec, action_spec)\n\u001b[0;32m----> 6\u001b[0m my_actor_policy \u001b[38;5;241m=\u001b[39m \u001b[43mactor_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mActorPolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_step_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_net\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Scrivania/Programming/ReinforcementLearning/env/lib/python3.10/site-packages/gin/config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m scope_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in scope \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(scope_str) \u001b[38;5;28;01mif\u001b[39;00m scope_str \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1604\u001b[0m err_str \u001b[38;5;241m=\u001b[39m err_str\u001b[38;5;241m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[0;32m-> 1605\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_exception_message_and_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Scrivania/Programming/ReinforcementLearning/env/lib/python3.10/site-packages/gin/utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m proxy \u001b[38;5;241m=\u001b[39m ExceptionProxy()\n\u001b[1;32m     40\u001b[0m ExceptionProxy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(exception)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m proxy\u001b[38;5;241m.\u001b[39mwith_traceback(exception\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Scrivania/Programming/ReinforcementLearning/env/lib/python3.10/site-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m new_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m   err_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Scrivania/Programming/ReinforcementLearning/env/lib/python3.10/site-packages/tf_agents/policies/actor_policy.py:113\u001b[0m, in \u001b[0;36mActorPolicy.__init__\u001b[0;34m(self, time_step_spec, action_spec, actor_network, policy_state_spec, info_spec, observation_normalizer, clip, training, observation_and_action_constraint_splitter, name)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    107\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_network must be a network.Network. Found \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    108\u001b[0m           \u001b[38;5;28mtype\u001b[39m(actor_network)\n\u001b[1;32m    109\u001b[0m       )\n\u001b[1;32m    110\u001b[0m   )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Create variables regardless of if we use the output spec.\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m actor_output_spec \u001b[38;5;241m=\u001b[39m \u001b[43mactor_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_variables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_step_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(actor_network, network\u001b[38;5;241m.\u001b[39mDistributionNetwork):\n\u001b[1;32m    118\u001b[0m   actor_output_spec \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    119\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m o: o\u001b[38;5;241m.\u001b[39msample_spec, actor_network\u001b[38;5;241m.\u001b[39moutput_spec\n\u001b[1;32m    120\u001b[0m   )\n",
      "File \u001b[0;32m~/Scrivania/Programming/ReinforcementLearning/env/lib/python3.10/site-packages/tf_agents/networks/network.py:223\u001b[0m, in \u001b[0;36mNetwork.create_variables\u001b[0;34m(self, input_tensor_spec, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_initial_state(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    222\u001b[0m step_type \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfill((\u001b[38;5;241m1\u001b[39m,), time_step\u001b[38;5;241m.\u001b[39mStepType\u001b[38;5;241m.\u001b[39mFIRST)\n\u001b[0;32m--> 223\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calc_unbatched_spec\u001b[39m(x):\n\u001b[1;32m    228\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Build Network output spec by removing previously added batch dimension.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m    Specs without batch dimension representing x.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Scrivania/Programming/ReinforcementLearning/env/lib/python3.10/site-packages/tf_agents/networks/network.py:440\u001b[0m, in \u001b[0;36mNetwork.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mis_tensor(network_state)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m network_state \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, ())\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_state\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m call_argspec\u001b[38;5;241m.\u001b[39margs\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m call_argspec\u001b[38;5;241m.\u001b[39mkeywords\n\u001b[1;32m    437\u001b[0m ):\n\u001b[1;32m    438\u001b[0m   normalized_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 440\u001b[0m outputs, new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(Network, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnormalized_kwargs)  \u001b[38;5;66;03m# pytype: disable=attribute-error  # typed-keras\u001b[39;00m\n\u001b[1;32m    442\u001b[0m nest_utils\u001b[38;5;241m.\u001b[39massert_matching_dtypes_and_inner_shapes(\n\u001b[1;32m    443\u001b[0m     new_state,\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_spec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     specs_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`state_spec`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, new_state\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)\n  In call to configurable 'ActorPolicy' (<class 'tf_agents.policies.actor_policy.ActorPolicy'>)"
     ]
    }
   ],
   "source": [
    "input_tensor_spec = tensor_spec.TensorSpec((4,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((3,), tf.float32, minimum=-1, maximum=1)\n",
    "action_net = ActionNet(input_tensor_spec, action_spec)\n",
    "\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_net\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
