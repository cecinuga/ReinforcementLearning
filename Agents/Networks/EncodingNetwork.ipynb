{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 16:52:20.266337: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-08 16:52:20.270122: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-08 16:52:20.344135: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-08 16:52:20.344204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-08 16:52:20.346009: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-08 16:52:20.355632: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-08 16:52:20.356434: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-08 16:52:22.268186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import random_py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import encoding_network\n",
    "from tf_agents.networks import network\n",
    "from tf_agents.networks import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.utils import common as common_utils\n",
    "from tf_agents.utils import nest_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Networks\n",
    "To create your own networks you will only have to override the __init__ and call methods. Let's create a custom network using what we learned about EncodingNetworks to create an ActorNetwork that takes observations which contain an image and a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(network.Network):\n",
    "    \"\"\"\n",
    "    Initializes the ActorNetwork to process observations and project them to actions.\n",
    "    Args:\n",
    "        observation_spec: A nested structure representing the specifications of the\n",
    "            observation tensor(s). It provides details such as shape and data type.\n",
    "        action_spec: A nested structure representing the specifications of the action\n",
    "            space. Must contain only a single floating-point action.\n",
    "        preprocessing_layers (optional): A sequence or list of layers (or layer\n",
    "            constructors) to be applied to the observation(s) prior to any other feature\n",
    "            processing. Defaults to None.\n",
    "        preprocessing_combiner (optional): A Keras layer dedicated to combining\n",
    "            the outputs of the preprocessing layers. Defaults to None.\n",
    "        conv_layer_params (optional): A tuple or list of tuples where each tuple\n",
    "            denotes (filters, kernel_size, stride) for a convolutional layer.\n",
    "            Defaults to None (no convolutional layers).\n",
    "        fc_layer_params (optional): A tuple of fully connected layer sizes to apply\n",
    "            after any convolutional layers. Defaults to (75, 40).\n",
    "        dropout_layer_params (optional): A list of float values specifying the rate for\n",
    "            dropout layers that follow each fully connected layer. Defaults to None.\n",
    "        activation_fn (optional): Activation function (callable) to use in the network.\n",
    "            Defaults to tf.keras.activations.relu.\n",
    "        enable_last_layer_zero_initializer (optional): If True, enables zero\n",
    "            initialization on the last fully connected layer (not the action projection\n",
    "            layer). Defaults to False.\n",
    "        name (optional): String name of the network. Defaults to 'ActorNetwork'.\n",
    "    Raises:\n",
    "        ValueError: If the action specification contains more than one action or if\n",
    "            the action data type is not floating-point.\n",
    "    This network builds an internal encoder to transform the observation(s)\n",
    "    into feature representations and then applies a final projection layer\n",
    "    to output actions in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                observation_spec,\n",
    "                action_spec,\n",
    "                preprocessing_layers=None,\n",
    "                preprocessing_combiner=None,\n",
    "                conv_layer_params=None,\n",
    "                fc_layer_params=(75, 40),\n",
    "                dropout_layer_params=None,\n",
    "                activation_fn=tf.keras.activations.relu,\n",
    "                enable_last_layer_zero_initializer=False,\n",
    "                name='ActorNetwork'):\n",
    "        \n",
    "        super(ActorNetwork, self).__init__(\n",
    "            input_tensor_spec=observation_spec, state_spec=(), name=name\n",
    "        )\n",
    "\n",
    "        self._action_spec = action_spec\n",
    "        flat_action_spec = tf.nest.flatten(action_spec)\n",
    "\n",
    "        if len(flat_action_spec) > 1:\n",
    "            raise ValueError('Only a single action is supported by this network')\n",
    "        self._single_action_spec = flat_action_spec[0]\n",
    "\n",
    "        if self._single_action_spec.dtype not in [ tf.float32, tf.float64 ]:\n",
    "            raise ValueError('Only float actions are supported by this network.')\n",
    "        \n",
    "        kernel_initializer = tf.keras.initializers.VarianceScaling(\n",
    "            scale=1./3., mode='fan_in', distribution='uniform'\n",
    "        )\n",
    "\n",
    "        self._encoder = encoding_network.EncodingNetwork(\n",
    "            observation_spec,\n",
    "            preprocessing_layers=preprocessing_layers,\n",
    "            preprocessing_combiner=preprocessing_combiner,\n",
    "            conv_layer_params=conv_layer_params,\n",
    "            fc_layer_params=fc_layer_params,\n",
    "            dropout_layer_params=dropout_layer_params,\n",
    "            activation_fn=activation_fn,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            batch_squash=False\n",
    "        )\n",
    "\n",
    "        initializer = tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.003, maxval=0.003\n",
    "        )\n",
    "\n",
    "        self._action_projection_layer = tf.keras.layers.Dense(\n",
    "            flat_action_spec[0].shape.num_elements(),\n",
    "            activation=tf.keras.activations.tanh,\n",
    "            kernel_initializer=initializer,\n",
    "            name='action'\n",
    "        )\n",
    "\n",
    "    def call(self, observation, step_type=(), network_state=()):\n",
    "        outer_rank = nest_utils.get_outer_rank(observation, self.input_tensor_spec)\n",
    "        batch_squash = utils.BatchSquash(outer_rank)\n",
    "        observations = tf.nest.map_structure(batch_squash.flatten, observation)\n",
    "\n",
    "        state, network_state = self._encoder(\n",
    "            observations, step_type=step_type, network_state=network_state\n",
    "        )\n",
    "        actions = self._action_projection_layer(state)\n",
    "        actions = common_utils.scale_to_spec(actions, self._single_action_spec)\n",
    "        actions = batch_squash.unflatten(actions)\n",
    "\n",
    "        return tf.nest.pack_sequence_as(self._action_spec, [actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
